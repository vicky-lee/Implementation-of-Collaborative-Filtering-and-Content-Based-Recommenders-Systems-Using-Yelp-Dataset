{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSC 575 Final Project - Vicky Lee / Implementation of Collaborative Filtering & Content-Based Recommender Systems Using Yelp Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of Yelp Dataset to Reviews for Restaurants in Toronto with User, Item, Rating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing of Yelp Dataset\n",
    "\n",
    "# Set directory to Yelp dataset folder\n",
    "import os\n",
    "os.chdir(\"Documents/yelp/\")\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from __future__ import division\n",
    "import time\n",
    "\n",
    "# Read review dataset\n",
    "review = pd.read_csv(\"yelp_academic_dataset_review.csv\")\n",
    "# review.shape\n",
    "\n",
    "# Count number of unqiue users and items for review dataset\n",
    "n_users_review = review.user_id.unique().shape[0]\n",
    "n_items_review = review.business_id.unique().shape[0]\n",
    "# n_users_review\n",
    "# n_items_review\n",
    "\n",
    "# Read business dataset\n",
    "business = pd.read_csv(\"yelp_academic_dataset_business.csv\")\n",
    "# business.shape\n",
    "\n",
    "# Group businesses by 'city'\n",
    "city = business.groupby('city')['city'].count()\n",
    "# city.sort_values(ascending=False)\n",
    "\n",
    "# Subset business to category 'Restaurants'\n",
    "restaurant = business[business['categories'].str.contains(\"Restaurants\",na=False)]\n",
    "# restaurant.shape\n",
    "# business['categories'].isnull().values.ravel().sum()\n",
    "\n",
    "# Group restaurant businesses by city\n",
    "city2 = restaurant.groupby('city')['city'].count()\n",
    "# city2.sort_values(ascending=False)\n",
    "\n",
    "# Filter restaurant businesses to city 'Toronto'\n",
    "restaurant_toronto = restaurant.loc[restaurant['city'] == 'Toronto']\n",
    "# restaurant_toronto.shape\n",
    "\n",
    "# Left join restaruants in toronto table with review table\n",
    "review_rest_tor = pd.merge(restaurant_toronto, review, on='business_id', how='left')\n",
    "# review_rest_tor.shape\n",
    "# review_rest_tor.columns\n",
    "\n",
    "# Subset to user, item, rating columns\n",
    "uir = review_rest_tor[['user_id','business_id','stars_y']]\n",
    "# len(uir)\n",
    "\n",
    "# Assign index for user and item\n",
    "user_index = uir.user_id.unique()\n",
    "item_index = uir.business_id.unique()\n",
    "\n",
    "# Count number of unique users and items\n",
    "n_users = uir.user_id.unique().shape[0]\n",
    "n_items = uir.business_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create User, Item Matrix and Filter down the dataset to 5 Core with every user and item with at least 5 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split User, Item, Rating dataset to train and test sets of 70% & 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(uir, test_size=0.30, random_state=42)\n",
    "# len(train)\n",
    "# len(test)\n",
    "\n",
    "# Create table for train data with list of users as index & items as columns\n",
    "train_matrix = pd.DataFrame(index=user_index, columns=item_index)\n",
    "# train_matrix.shape\n",
    "\n",
    "# Fill in train_matrix table with ratings\n",
    "for row in train.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    train_matrix.loc[user][item] = row[3]  \n",
    "\n",
    "# Create table for test data with list of users as index & items as columns    \n",
    "test_matrix = pd.DataFrame(index=user_index, columns=item_index)\n",
    "# test_matrix.shape\n",
    "\n",
    "# Fill in test_matrix table with ratings\n",
    "for row in test.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    test_matrix.loc[user][item] = row[3]\n",
    "\n",
    "# Begin filtering process to create 5 Core Subset\n",
    "\n",
    "# Count number of rated items for each user\n",
    "item_1 = train_matrix.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_1.value_counts()\n",
    "\n",
    "# Filter down to the users with greater than or equal to 5 ratings\n",
    "train1 = train_matrix\n",
    "train1['item_1'] = item_1\n",
    "train2 = train1.loc[train1['item_1'] >= 5]\n",
    "# train2.shape\n",
    "\n",
    "# Count number of rated users for each item\n",
    "train2 = train2.drop('item_1',axis=1)\n",
    "train3 = train2.transpose()\n",
    "user_1 = train3.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_1.value_counts()\n",
    "\n",
    "# Filter down to the items with greater than or equal to 5 ratings\n",
    "train3['user_1'] = user_1\n",
    "train4 = train3.loc[train3['user_1'] >= 5]\n",
    "train4 = train4.drop('user_1',axis=1)\n",
    "train5 = train4.transpose()\n",
    "# train5.shape\n",
    "\n",
    "# Repeat the process for both user and item\n",
    "item_2 = train5.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_2.value_counts()\n",
    "# item_2.shape\n",
    "train5['item_2'] = item_2\n",
    "train6 = train5.loc[train5['item_2'] >= 5]\n",
    "train6 = train6.drop('item_2',axis=1)\n",
    "train7 = train6.transpose()\n",
    "user_2 = train7.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_2.value_counts()\n",
    "train7['user_2'] = user_2\n",
    "train8 = train7.loc[train7['user_2'] >= 5]\n",
    "train8 = train8.drop('user_2',axis=1)\n",
    "train9 = train8.transpose()\n",
    "# train9.shape\n",
    "\n",
    "# Check every user and item has at least 5 ratings\n",
    "item_3 = train9.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_3.value_counts()\n",
    "user_3 = train9.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_3.value_counts()\n",
    "\n",
    "# Filter down the test matrix to filtered user and item in train matrix\n",
    "test9 = test_matrix.loc[train9.index,train9.columns]\n",
    "# test9.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process for Restaurants in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter restaurant businesses to city 'Phoenix'\n",
    "restaurant_phoenix = restaurant.loc[restaurant['city'] == 'Phoenix']\n",
    "\n",
    "# restaurant_phoenix.shape\n",
    "\n",
    "# Left join restaruants in phoenix table with review table\n",
    "review_rest_pho = pd.merge(restaurant_phoenix, review, on='business_id', how='left')\n",
    "# review_rest_pho.shape\n",
    "# review_rest_pho.columns\n",
    "\n",
    "# Subset to user, item, rating columns\n",
    "uir_ph = review_rest_pho[['user_id','business_id','stars_y']]\n",
    "# len(uir_ph)\n",
    "\n",
    "# Assign index for user and item\n",
    "user_index_ph = uir_ph.user_id.unique()\n",
    "item_index_ph = uir_ph.business_id.unique()\n",
    "\n",
    "# Count number of unique users and items\n",
    "n_users_ph = uir_ph.user_id.unique().shape[0]\n",
    "n_items_ph = uir_ph.business_id.unique().shape[0]\n",
    "\n",
    "# Split User, Item, Rating dataset to train and test sets of 70% & 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ph, test_ph = train_test_split(uir_ph, test_size=0.30, random_state=42)\n",
    "# len(train_ph)\n",
    "# len(test_ph)\n",
    "\n",
    "# Create table for train data with list of users as index & items as columns\n",
    "train_matrix_ph = pd.DataFrame(index=user_index_ph, columns=item_index_ph)\n",
    "# train_matrix_ph.shape\n",
    "\n",
    "# Fill in train_matrix table with ratings\n",
    "for row in train_ph.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    train_matrix_ph.loc[user][item] = row[3]  \n",
    "\n",
    "# Create table for test data with list of users as index & items as columns    \n",
    "test_matrix_ph = pd.DataFrame(index=user_index_ph, columns=item_index_ph)\n",
    "# test_matrix_ph.shape\n",
    "\n",
    "# Fill in test_matrix table with ratings\n",
    "for row in test_ph.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    test_matrix_ph.loc[user][item] = row[3]\n",
    "\n",
    "# Begin filtering process to create 5 Core Subset\n",
    "\n",
    "# Count number of rated items for each user\n",
    "item_1_ph = train_matrix_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_1_ph.value_counts()\n",
    "\n",
    "# Filter down to the users with greater than or equal to 5 ratings\n",
    "train1_ph = train_matrix_ph\n",
    "train1_ph['item_1'] = item_1_ph\n",
    "train2_ph = train1_ph.loc[train1_ph['item_1'] >= 5]\n",
    "# train2_ph.shape\n",
    "\n",
    "# Count number of rated users for each item\n",
    "train2_ph = train2_ph.drop('item_1',axis=1)\n",
    "train3_ph = train2_ph.transpose()\n",
    "user_1_ph = train3_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_1_ph.value_counts()\n",
    "\n",
    "# Filter down to the items with greater than or equal to 5 ratings\n",
    "train3_ph['user_1'] = user_1_ph\n",
    "train4_ph = train3_ph.loc[train3_ph['user_1'] >= 5]\n",
    "train4_ph = train4_ph.drop('user_1',axis=1)\n",
    "train5_ph = train4_ph.transpose()\n",
    "# train5_ph.shape\n",
    "\n",
    "# Repeat the process for both user and item\n",
    "item_2_ph = train5_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_2_ph.value_counts()\n",
    "\n",
    "train5_ph['item_2'] = item_2_ph\n",
    "train6_ph = train5_ph.loc[train5_ph['item_2'] >= 5]\n",
    "train6_ph = train6_ph.drop('item_2',axis=1)\n",
    "train7_ph = train6_ph.transpose()\n",
    "user_2_ph = train7_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_2_ph.value_counts()\n",
    "train7_ph['user_2'] = user_2_ph\n",
    "train8_ph = train7_ph.loc[train7_ph['user_2'] >= 5]\n",
    "train8_ph = train8_ph.drop('user_2',axis=1)\n",
    "train9_ph = train8_ph.transpose()\n",
    "# train9_ph.shape\n",
    "\n",
    "item_3_ph = train9_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_3_ph.value_counts()\n",
    "\n",
    "user_3_ph = train9_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_3_ph.value_counts()\n",
    "\n",
    "# Filter down the test matrix to filtered user and item in train matrix\n",
    "test9_ph = test_matrix_ph.loc[train9_ph.index,train9_ph.columns]\n",
    "# test9_ph.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Yelp dataset & User, Item, Rating dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yelp Review Dataset\n",
      "Size of original Yelp dataset, Review is (4153150, 10) \n",
      "Number of users is 1029432, number of items is 144072 \n",
      "\n",
      "Yelp Business Dataset\n",
      "Size of original Yelp dataset, Business is (144072, 16) \n",
      "Size of Restaurants subset of Business dataset is (48485, 16) \n",
      "\n",
      "Top 10 Cities with the most number of restaurants are as follows \n",
      "city\n",
      "Toronto        6347\n",
      "Las Vegas      5431\n",
      "Phoenix        3353\n",
      "Montréal       2852\n",
      "Charlotte      2201\n",
      "Pittsburgh     1990\n",
      "Edinburgh      1412\n",
      "Scottsdale     1356\n",
      "Cleveland      1235\n",
      "Mississauga    1128\n",
      "Name: city, dtype: int64 \n",
      "\n",
      "Restaurants in Toronto Subset\n",
      "Size of Restaurants in Toronto subset of Business dataset is (6347, 16) \n",
      "User, Item, Rating dataset for restuarnats in Toronto contain 245127 rows \n",
      "Number of users is 58355, number of items is 6347 \n",
      "\n",
      "Restaurants in Phoenix Subset\n",
      "Size of Restaurants in Phoenix subset of Business dataset is (3353, 16) \n",
      "User, Item, Rating dataset for restuarnats in Toronto contain 266766 rows \n",
      "Number of users is 97476, number of items is 3353 \n"
     ]
    }
   ],
   "source": [
    "print '\\nYelp Review Dataset'\n",
    "print 'Size of original Yelp dataset, Review is %s ' % str(review.shape)\n",
    "print 'Number of users is %s, number of items is %s ' % (str(n_users_review),str(n_items_review))\n",
    "print '\\nYelp Business Dataset'\n",
    "print 'Size of original Yelp dataset, Business is %s ' % str(business.shape)\n",
    "print 'Size of Restaurants subset of Business dataset is %s ' % str(restaurant.shape)\n",
    "print '\\nTop 10 Cities with the most number of restaurants are as follows \\n%s ' % str(city2.sort_values(ascending=False).head(10))\n",
    "print '\\nRestaurants in Toronto Subset'\n",
    "print 'Size of Restaurants in Toronto subset of Business dataset is %s ' % str(restaurant_toronto.shape)\n",
    "print 'User, Item, Rating dataset for restuarnats in Toronto contain %s rows ' % str(len(uir))\n",
    "print 'Number of users is %s, number of items is %s ' % (str(n_users),str(n_items))\n",
    "print '\\nRestaurants in Phoenix Subset'\n",
    "print 'Size of Restaurants in Phoenix subset of Business dataset is %s ' % str(restaurant_phoenix.shape)\n",
    "print 'User, Item, Rating dataset for restuarnats in Toronto contain %s rows ' % str(len(uir_ph))\n",
    "print 'Number of users is %s, number of items is %s ' % (str(n_users_ph),str(n_items_ph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Test Matrices of Users, Items & 5 Core Subset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restaurants in Toronto\n",
      "\n",
      "Train dataset contains 171588 rows, Test dataset contains 73539 rows \n",
      "Size of train & test matrices with users in rows & items in columns is (58355, 6348) \n",
      "\n",
      "Users of filtered 5 core subset has rated at least 5 items as follows  \n",
      "5    1419\n",
      "6     946\n",
      "7     663\n",
      "8     568\n",
      "9     436\n",
      "dtype: int64\n",
      "\n",
      "Items of filtered 5 core subset has been rated by at least 5 users as follows  \n",
      "5    1419\n",
      "6     946\n",
      "7     663\n",
      "8     568\n",
      "9     436\n",
      "dtype: int64\n",
      "\n",
      "Size of 5 core subset is (7016, 3948) \n",
      "\n",
      "Restaurants in Phoenix\n",
      "\n",
      "Train dataset contains 186736 rows, Test dataset contains 80030 rows \n",
      "Size of train & test matrices with users in rows & items in columns is (97476, 3354) \n",
      "\n",
      "Size of 5 core subset is (7032, 2153) \n"
     ]
    }
   ],
   "source": [
    "print '\\nRestaurants in Toronto'\n",
    "print '\\nTrain dataset contains %s rows, Test dataset contains %s rows ' % (str(len(train)),str(len(test)))\n",
    "print 'Size of train & test matrices with users in rows & items in columns is %s ' % (str(train_matrix.shape))\n",
    "print '\\nUsers of filtered 5 core subset has rated at least 5 items as follows  \\n%s' % (str(item_3.value_counts().head(5)))\n",
    "print '\\nItems of filtered 5 core subset has been rated by at least 5 users as follows  \\n%s' % (str(user_3.value_counts().head(5)))\n",
    "print '\\nSize of 5 core subset is %s ' % (str(train9.shape))\n",
    "print '\\nRestaurants in Phoenix'\n",
    "print '\\nTrain dataset contains %s rows, Test dataset contains %s rows ' % (str(len(train_ph)),str(len(test_ph)))\n",
    "print 'Size of train & test matrices with users in rows & items in columns is %s ' % (str(train_matrix_ph.shape))\n",
    "print '\\nSize of 5 core subset is %s ' % (str(train9_ph.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood-Based Collaborative Filtering - User Based & Item Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collaborative_filtering(train, test, sim='cosine', type='user', knn=5):\n",
    "\n",
    "    # Fill NAN values in train & test data as 0's\n",
    "    train_0 = train.fillna(0)\n",
    "    test_0 = test.fillna(0)\n",
    "\n",
    "    # Create a similarity matrix of either users or items based on cosine or \n",
    "    # pearson correlation measure\n",
    "    if sim == 'cosine':\n",
    "        user_dist = pairwise_distances(train_0, metric='cosine')\n",
    "        item_dist = pairwise_distances(train_0.T, metric='cosine')\n",
    "\n",
    "    elif sim == 'pearson':\n",
    "        user_dist = pairwise_distances(train_0, metric='correlation')\n",
    "        item_dist = pairwise_distances(train_0.T, metric='correlation')\n",
    "        \n",
    "    user_sim = 1 - user_dist\n",
    "    item_sim = 1 - item_dist\n",
    " \n",
    "    # Create a dataframe with mean user ratings\n",
    "    mean_rating = train.mean(axis=1)\n",
    "    mean_user_rating = pd.concat([mean_rating] * len(train.columns), axis=1)\n",
    "    mean_user_rating.columns = train.columns\n",
    "    \n",
    "    # Modify a dataframe so that mean user ratings are present only in matrix \n",
    "    # positions of rated items and 0's in matrix positions of non-rated items\n",
    "    mean_user_rating_0 = mean_user_rating\n",
    "    mean_user_rating_0[train_0 == 0] = 0\n",
    "    \n",
    "    # Normalize every user's ratings to mean of zero\n",
    "    ratings_diff = train_0 - mean_user_rating_0\n",
    "    \n",
    "    # Create a dataframe with user's mean user ratings present in all items\n",
    "    mean_user_rating_f = pd.concat([mean_rating] * len(train.columns), axis=1)\n",
    "    mean_user_rating_f.columns = train.columns\n",
    "    \n",
    "    # Create a placeholder dataframe for predictions of rated items in test data\n",
    "    pred = pd.DataFrame(index=train.index, columns=train.columns)\n",
    "    \n",
    "    # User-Based Collaborative Filtering\n",
    "    if type == 'user':    \n",
    "        # Index user similarity matrix with user ids for both rows and columns\n",
    "        user_sim = pd.DataFrame(user_sim, index=train.index, columns=train.index)\n",
    "        # When the number of k neareast neighbors is specified\n",
    "        if knn != 'all':\n",
    "            user_sim_mat = user_sim.as_matrix()\n",
    "        \n",
    "            # Item id's of rated items in test data for each user\n",
    "            cols = test.columns\n",
    "            test_rated = test.apply(lambda x: x > 0, raw=True).apply(lambda x: list(cols[x.values]), axis=1)\n",
    "            \n",
    "            # Iterate over each user, m\n",
    "            for m in range(len(user_sim)):\n",
    "                # Retrieve column of user m's similarities to all other users\n",
    "                temp = user_sim_mat[m]\n",
    "                temp = pd.DataFrame(temp,index=train.index, columns=['similarity'])\n",
    "                # Rank user m's similarities\n",
    "                temp['rank'] = temp['similarity'].rank(ascending=0)\n",
    "                \n",
    "                # Iterate over user m's rated items\n",
    "                for n in range(len(test_rated[m])):\n",
    "                    # For user m's nth rated item, extract column of ratings of \n",
    "                    # all users corresponding to nth item\n",
    "                    temp2 = ratings_diff[[test_rated[m][n]]]\n",
    "                    temp2.columns = ['rating']\n",
    "                    # Contatenate similarity, rank, rating as one dataframe\n",
    "                    result = pd.concat([temp, temp2], axis=1)    \n",
    "                    # Filter down to the users who rated the items\n",
    "                    result2 = result[result['rating'] != 0]\n",
    "                    # Filter down to knn number of users with the knn \n",
    "                    # highest similairites\n",
    "                    result3 = result2.nsmallest(int(knn), 'rank')\n",
    "                \n",
    "                    # Divide weighted sum of user's knn nearest neighbors' ratings by \n",
    "                    # sum of their similarities\n",
    "                    score = result3['similarity'].dot(result3['rating'])/result3['similarity'].sum()\n",
    "                    mean = mean_user_rating_f.loc[train.index[m]][test_rated[m][n]]\n",
    "                \n",
    "                    # Make a prediction by adding user's mean rating to weighted sum\n",
    "                    pred.loc[train.index[m]][test_rated[m][n]] = mean + score\n",
    "                    \n",
    "        # When the number of k neareast neighbors is not specified and all \n",
    "        # available neighbors are used for prediction     \n",
    "        elif knn == 'all':  \n",
    "            # Compute user similarity weighted sum of available ratings of \n",
    "            # user's every neighbor \n",
    "            num_user = user_sim.dot(ratings_diff)\n",
    "            \n",
    "            # Sum user similarities    \n",
    "            sum_sim_user = user_sim.sum(axis=1)\n",
    "            sum_sim_mat_user = pd.concat([sum_sim_user] * len(train.columns), axis=1)\n",
    "            sum_sim_mat_user.columns = train.columns\n",
    "\n",
    "            # Create a dataframe of predictions computed by adding mean user \n",
    "            # rating to user similairty weighted sum of user's ratings \n",
    "            # divided by sum of user similarities\n",
    "            pred = mean_user_rating_f + num_user / sum_sim_mat_user\n",
    "                \n",
    "    # Item-Based Collaborative Filtering  \n",
    "    elif type == 'item':\n",
    "        # Index item similarity matrix with item ids for both rows and columns\n",
    "        item_sim = pd.DataFrame(item_sim, index=train.columns, columns=train.columns)\n",
    "        \n",
    "        # When the number of k neareast neighbors is specified\n",
    "        if knn != 'all':\n",
    "            item_sim_mat = item_sim.as_matrix()\n",
    "\n",
    "            # User id's of rated items in test data for each item\n",
    "            cols2 = test.T.columns\n",
    "            test_rated2 = test.T.apply(lambda x: x > 0, raw=True).apply(lambda x: list(cols2[x.values]), axis=1)\n",
    "        \n",
    "            # Iterate over each item, m\n",
    "            for m in range(len(item_sim)):\n",
    "                # Retrieve column of item m's similarities to all other items\n",
    "                temp = item_sim_mat[m]\n",
    "                temp = pd.DataFrame(temp,index=train.columns, columns=['similarity'])\n",
    "                # Rank item m's similarities\n",
    "                temp['rank'] = temp['similarity'].rank(ascending=0)\n",
    "\n",
    "                # Iterate over item m's rated users\n",
    "                for n in range(len(test_rated2[m])):\n",
    "                    # For item m's nth rated user, extract column of ratings of \n",
    "                    # all items corresponding to nth user\n",
    "                    temp2 = ratings_diff.T[[test_rated2[m][n]]]\n",
    "                    temp2.columns = ['rating']\n",
    "                    # Contatenate similarity, rank, rating as one dataframe\n",
    "                    result = pd.concat([temp, temp2], axis=1)    \n",
    "                    # Filter down to the items that are rated by the user\n",
    "                    result2 = result[result['rating'] != 0]\n",
    "                    # Filter down to knn number of items with the knn \n",
    "                    # highest similairites\n",
    "                    result3 = result2.nsmallest(int(knn), 'rank')\n",
    "                \n",
    "                    # Divide weighted sum of item's knn nearest neighbors' \n",
    "                    # ratings by sum of their similarities\n",
    "                    score = result3['similarity'].dot(result3['rating'])/result3['similarity'].sum()\n",
    "                    mean = mean_user_rating_f.loc[test_rated2[m][n]][train.columns[m]]\n",
    "                \n",
    "                    # Make a prediction by adding user's mean rating to weighted sum\n",
    "                    pred.loc[test_rated2[m][n]][train.columns[m]] = mean + score\n",
    "        \n",
    "        # When the number of k neareast neighbors is not specified and \n",
    "        # all available neighbors are used for prediction\n",
    "        elif knn == 'all':\n",
    "            # Compute item similarity weighted sum of available ratings of \n",
    "            # item's every neighbor\n",
    "            num_item = ratings_diff.dot(item_sim)\n",
    "\n",
    "            # Sum item similarities \n",
    "            sum_sim_item = item_sim.sum(axis=1)\n",
    "            sum_sim_item = pd.DataFrame(sum_sim_item, index=train.columns)\n",
    "            sum_sim_mat_item = pd.concat([sum_sim_item.T] * len(train), axis=0)\n",
    "\n",
    "            sum_sim_mat_item.index = train.index\n",
    "\n",
    "            # Create a dataframe of predictions computed by adding mean user \n",
    "            # rating to item similairty weighted sum of items' ratings \n",
    "            # divided by sum of item similarities\n",
    "            pred = mean_user_rating_f + num_item / sum_sim_mat_item\n",
    "            \n",
    "    pred_0 = pred.fillna(0)\n",
    "    pred_0_mat = pred_0.as_matrix()            \n",
    "    return pred_0_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test9_0 = test9.fillna(0)\n",
    "test9_0_mat = test9_0.as_matrix()\n",
    "test9_0_ph = test9_ph.fillna(0)\n",
    "test9_0_mat_ph = test9_0_ph.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error Computation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "def mae(prediction, actual):\n",
    "    prediction = prediction[actual.nonzero()].flatten() \n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_absolute_error(prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Collaborative Filtering on Restaurants in Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:76: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:136: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Runs of Collaborative Filtering for Restaurant in Toronto took 2022.19950795 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_user_5 = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='5')\n",
    "pred_user_10 = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='10')\n",
    "pred_user_20 = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='20')\n",
    "pred_user_all = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='all')\n",
    "pred_item_5 = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='5')\n",
    "pred_item_10 = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='10')\n",
    "pred_item_20 = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='20')\n",
    "pred_item_all = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='all')\n",
    "print '8 Runs of Collaborative Filtering for Restaurant in Toronto took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Runs of Collaborative Filtering for Restaurant in Toronto took 552.532515049 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_user_cosine = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='all')\n",
    "pred_user_pearson = collaborative_filtering(train9, test9, sim='pearson', type='user', knn='all')\n",
    "pred_item_cosine = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='all')\n",
    "pred_item_pearson = collaborative_filtering(train9, test9, sim='pearson', type='item', knn='all')\n",
    "print '4 Runs of Collaborative Filtering for Restaurant in Toronto took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Collaborative Filtering on Restaurants in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:76: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:136: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Runs of Collaborative Filtering for Restaurant in Phoenix took 1469.81145906 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_user_5_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='user', knn='5')\n",
    "pred_user_10_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='user', knn='10')\n",
    "pred_user_20_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='user', knn='20')\n",
    "pred_user_all_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='user', knn='all')\n",
    "pred_item_5_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='item', knn='5')\n",
    "pred_item_10_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='item', knn='10')\n",
    "pred_item_20_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='item', knn='20')\n",
    "pred_item_all_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='item', knn='all')\n",
    "print '8 Runs of Collaborative Filtering for Restaurant in Phoenix took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Runs of Collaborative Filtering for Restaurant in Phoenix took 259.879482031 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_user_cosine_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='user', knn='all')\n",
    "pred_user_pearson_ph = collaborative_filtering(train9_ph, test9_ph, sim='pearson', type='user', knn='all')\n",
    "pred_item_cosine_ph = collaborative_filtering(train9_ph, test9_ph, sim='cosine', type='item', knn='all')\n",
    "pred_item_pearson_ph = collaborative_filtering(train9_ph, test9_ph, sim='pearson', type='item', knn='all')\n",
    "print '4 Runs of Collaborative Filtering for Restaurant in Phoenix took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Restaurants in Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurnats in Toronto with Varying Number of Nearest Neighbors\n",
      "User-based CF using 5 nearest neighbors MAE: 0.9265\n",
      "User-based CF using 10 nearest neighbors MAE: 0.9044\n",
      "User-based CF using 20 nearest neighbors MAE: 0.8971\n",
      "User-based CF using All available neighbors MAE: 0.8717\n",
      "Item-based CF using 5 nearest neighbors MAE: 0.988\n",
      "Item-based CF using 10 nearest neighbors MAE: 0.9705\n",
      "Item-based CF using 20 nearest neighbors MAE: 0.9666\n",
      "Item-based CF using All available neighbors MAE: 0.8734\n"
     ]
    }
   ],
   "source": [
    "print 'Restaurnats in Toronto with Varying Number of Nearest Neighbors'\n",
    "print 'User-based CF using 5 nearest neighbors MAE: ' + str(round(mae(pred_user_5, test9_0_mat),4))\n",
    "print 'User-based CF using 10 nearest neighbors MAE: ' + str(round(mae(pred_user_10, test9_0_mat),4))\n",
    "print 'User-based CF using 20 nearest neighbors MAE: ' + str(round(mae(pred_user_20, test9_0_mat),4))\n",
    "print 'User-based CF using All available neighbors MAE: ' + str(round(mae(pred_user_all, test9_0_mat),4))\n",
    "print 'Item-based CF using 5 nearest neighbors MAE: ' + str(round(mae(pred_item_5, test9_0_mat),4))\n",
    "print 'Item-based CF using 10 nearest neighbors MAE: ' + str(round(mae(pred_item_10, test9_0_mat),4))\n",
    "print 'Item-based CF using 20 nearest neighbors MAE: ' + str(round(mae(pred_item_20, test9_0_mat),4))\n",
    "print 'Item-based CF using All available neighbors MAE: ' + str(round(mae(pred_item_all, test9_0_mat),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurnats in Toronto Using Cosine Similarity or Pearson Correlation\n",
      "User-based CF using Cosine Similarity MAE: 0.8717\n",
      "User-based CF using Pearson Correlation MAE: 0.8724\n",
      "Item-based CF using Cosine Similarity MAE: 0.8734\n",
      "Item-based CF using Pearson Correlation MAE: 0.8747\n"
     ]
    }
   ],
   "source": [
    "print 'Restaurnats in Toronto Using Cosine Similarity or Pearson Correlation'\n",
    "print 'User-based CF using Cosine Similarity MAE: ' + str(round(mae(pred_user_cosine, test9_0_mat),4))\n",
    "print 'User-based CF using Pearson Correlation MAE: ' + str(round(mae(pred_user_pearson, test9_0_mat),4))\n",
    "print 'Item-based CF using Cosine Similarity MAE: ' + str(round(mae(pred_item_cosine, test9_0_mat),4))\n",
    "print 'Item-based CF using Pearson Correlation MAE: ' + str(round(mae(pred_item_pearson, test9_0_mat),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Restaurant in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurnats in Phoenix with Varying Number of Nearest Neighbors\n",
      "User-based CF using 5 nearest neigbors MAE: 0.9366\n",
      "User-based CF using 10 nearest neigbors MAE: 0.9091\n",
      "User-based CF using 20 nearest neigbors MAE: 0.8979\n",
      "User-based CF using All available neigbors MAE: 0.8832\n",
      "Item-based CF using 5 nearest neigbors MAE: 0.9908\n",
      "Item-based CF using 10 nearest neigbors MAE: 0.9747\n",
      "Item-based CF using 20 nearest neigbors MAE: 0.9699\n",
      "Item-based CF using All available neigbors MAE: 0.8846\n"
     ]
    }
   ],
   "source": [
    "print 'Restaurnats in Phoenix with Varying Number of Nearest Neighbors'\n",
    "print 'User-based CF using 5 nearest neigbors MAE: ' + str(round(mae(pred_user_5_ph, test9_0_mat_ph),4))\n",
    "print 'User-based CF using 10 nearest neigbors MAE: ' + str(round(mae(pred_user_10_ph, test9_0_mat_ph),4))\n",
    "print 'User-based CF using 20 nearest neigbors MAE: ' + str(round(mae(pred_user_20_ph, test9_0_mat_ph),4))\n",
    "print 'User-based CF using All available neigbors MAE: ' + str(round(mae(pred_user_all_ph, test9_0_mat_ph),4))\n",
    "print 'Item-based CF using 5 nearest neigbors MAE: ' + str(round(mae(pred_item_5_ph, test9_0_mat_ph),4))\n",
    "print 'Item-based CF using 10 nearest neigbors MAE: ' + str(round(mae(pred_item_10_ph, test9_0_mat_ph),4))\n",
    "print 'Item-based CF using 20 nearest neigbors MAE: ' + str(round(mae(pred_item_20_ph, test9_0_mat_ph),4))\n",
    "print 'Item-based CF using All available neigbors MAE: ' + str(round(mae(pred_item_all_ph, test9_0_mat_ph),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurnats in Phoenix Using Cosine Similarity or Pearson Correlation\n",
      "User-based CF using Cosine Similarity MAE: 0.8832\n",
      "User-based CF using Pearson Correlation MAE: 0.8951\n",
      "Item-based CF using Cosine Similarity MAE: 0.8846\n",
      "Item-based CF using Pearson Correlation MAE: 0.8859\n"
     ]
    }
   ],
   "source": [
    "print 'Restaurnats in Phoenix Using Cosine Similarity or Pearson Correlation'\n",
    "print 'User-based CF using Cosine Similarity MAE: ' + str(round(mae(pred_user_cosine_ph, test9_0_mat_ph),4))\n",
    "print 'User-based CF using Pearson Correlation MAE: ' + str(round(mae(pred_user_pearson_ph, test9_0_mat_ph),4))\n",
    "print 'Item-based CF using Cosine Similarity MAE: ' + str(round(mae(pred_item_cosine_ph, test9_0_mat_ph),4))\n",
    "print 'Item-based CF using Pearson Correlation MAE: ' + str(round(mae(pred_item_pearson_ph, test9_0_mat_ph),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features dataset for Restaurants in Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature = restaurant_toronto[['business_id','attributes','categories']]\n",
    "\n",
    "# Extract features from column, categories & attributes for items(restaurants in Toronto)\n",
    "feature1 = pd.DataFrame(feature['categories'].astype(str).str.split(',').tolist())\n",
    "feature1 = feature1.apply(lambda x: x.str.lower())\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('[',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "# Extract features from column, attributes for items(restaurants in Toronto)\n",
    "feature2 = pd.DataFrame(feature['attributes'].astype(str).str.split(',').tolist())\n",
    "feature2 = feature2.apply(lambda x: x.str.lower())\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace(r'.*: {',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('}\\\"',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace(':',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('[',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "feature1 = feature1.as_matrix()\n",
    "feature2 = feature2.as_matrix()\n",
    "features = np.concatenate((feature1, feature2),axis=1)\n",
    "features = pd.DataFrame(features)\n",
    "\n",
    "features.index = feature['business_id']\n",
    "filtered = list(train9.columns.values)\n",
    "features_filt = features[features.index.isin(filtered)]\n",
    "number_of_features = pd.unique(features_filt.values.ravel())\n",
    "# len(number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features dataset for Restaurants in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_ph = restaurant_phoenix[['business_id','attributes','categories']]\n",
    "\n",
    "# Extract features from column, categories & attributes for items(restaurants in Phoenix)\n",
    "feature1_ph = pd.DataFrame(feature_ph['categories'].astype(str).str.split(',').tolist())\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.lower())\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('[',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "# Extract features from column, attributes for items(restaurants in Phoenix)\n",
    "feature2_ph = pd.DataFrame(feature_ph['attributes'].astype(str).str.split(',').tolist())\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.lower())\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace(r'.*: {',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('}\\\"',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace(':',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('[',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "feature1_ph = feature1_ph.as_matrix()\n",
    "feature2_ph = feature2_ph.as_matrix()\n",
    "features_ph = np.concatenate((feature1_ph, feature2_ph),axis=1)\n",
    "features_ph = pd.DataFrame(features_ph)\n",
    "\n",
    "features_ph.index = feature_ph['business_id']\n",
    "filtered_ph = list(train9_ph.columns.values)\n",
    "features_filt_ph = features_ph[features_ph.index.isin(filtered_ph)]\n",
    "number_of_features_ph = pd.unique(features_filt_ph.values.ravel())\n",
    "# len(number_of_features_ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurants in Toronto\n",
      "\n",
      "There are 536 features\n",
      "Examples of features are as follows \n",
      "[' lunch false' ' dinner false' ' breakfast true' ' brunch true'\n",
      " ' hastv true' ' noiselevel average' ' restaurantspricerange2 1'\n",
      " ' restaurantsreservations true' ' restaurantstableservice true'\n",
      " ' wheelchairaccessible true' ' wifi free' 'restaurants' ' japanese'\n",
      " ' sushi bars' 'alcohol beer_and_wine' ' restaurantsdelivery true' 'korean'\n",
      " ' street true' ' caters false' ' goodforkids false'] \n",
      "\n",
      "Restaurants in Phoenix\n",
      "\n",
      "There are 476 features\n",
      "Examples of features are as follows \n",
      "[' kosher true' ' halal true' ' soy-free true' 'karaoke' 'latin american'\n",
      " ' cambodian' 'tapas/small plates' ' szechuan' ' local flavor'\n",
      " ' fruits & veggies' 'drugstores' 'bikeparking false' 'sports clubs'\n",
      " ' food stands' ' jazz & blues' 'caribbean' ' cuban' 'arts & entertainment'\n",
      " ' cupcakes' 'fondue'] \n"
     ]
    }
   ],
   "source": [
    "print 'Restaurants in Toronto'\n",
    "print '\\nThere are %s features' % str(len(number_of_features))\n",
    "print 'Examples of features are as follows \\n%s ' % str(number_of_features[50:70])\n",
    "print '\\nRestaurants in Phoenix'\n",
    "print '\\nThere are %s features' % str(len(number_of_features_ph))\n",
    "print 'Examples of features are as follows \\n%s ' % str(number_of_features_ph[350:370])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_based_recommender(train, test, features):\n",
    "\n",
    "    # Train Data\n",
    "    # For each user, retrieve item id's of rated items\n",
    "    col = train.columns\n",
    "    pref_known_train = train.apply(lambda x: x > 0, raw=True).apply(lambda x: list(col[x.values]), axis=1)\n",
    "\n",
    "    # Test Data\n",
    "    # For each user, retrieve item id's of rated items\n",
    "    cols = test.columns\n",
    "    pref_known_test = test.apply(lambda x: x > 0, raw=True).apply(lambda x: list(cols[x.values]), axis=1)\n",
    "\n",
    "    # Create a dataframe with mean user ratings\n",
    "    mean_rating = train.mean(axis=1)\n",
    "    mean_user_rating = pd.concat([mean_rating] * len(train.columns), axis=1)\n",
    "    mean_user_rating.columns = train.columns\n",
    "    \n",
    "    # Create a dataframe for test data prediction\n",
    "    test_pred = pd.DataFrame(index=test.index, columns=test.columns)\n",
    "\n",
    "    # For each user m, create a dataframe with columns, 'feature', 'like', 'dislike'\n",
    "    for m in range(len(train)):\n",
    "        like_m = pd.DataFrame(data=None, columns=['feature','like','dislike'])\n",
    "        like = 0\n",
    "        dislike = 0\n",
    "        total = 0\n",
    "        # Iterate over items with ratings\n",
    "        for x1 in pref_known_train[m]:\n",
    "            # Count the item, x1 as 'like' if the rating is greater than 2\n",
    "            if train.loc[train.index[m]][x1] > 2:\n",
    "                like = like + 1\n",
    "                # Iterate over features of item, x1 and append row with feature: z1, \n",
    "                # 1 for like, 0 for dislike\n",
    "                for z1 in np.array(features.loc[x1]):\n",
    "                    if z1 is not None:\n",
    "                        like_new1 = pd.DataFrame([[z1,1,0]],columns=['feature','like','dislike'])\n",
    "                        like_m = like_m.append(like_new1)\n",
    "                    \n",
    "            # Count the item, x1 as 'dislike' if the rating is less than 2\n",
    "            else:\n",
    "                dislike = dislike + 1\n",
    "                # Iterate over features of item, x1 and append row with feature: z1, \n",
    "                # 0 for like, 1 for dislike\n",
    "                for z1 in np.array(features.loc[x1]):\n",
    "                    if z1 is not None:\n",
    "                        like_new1 = pd.DataFrame([[z1,0,1]],columns=['feature','like','dislike'])\n",
    "                        like_m = like_m.append(like_new1)\n",
    "            total = total + 1\n",
    "    \n",
    "        # P(like) & P(dislike) for user m\n",
    "        prob_like =  like / total\n",
    "        prob_dislike = dislike / total\n",
    "        # Sum like & dislike counts by features\n",
    "        like_m_f = like_m.groupby(\"feature\").sum()\n",
    "\n",
    "        # Conditional probabilities, P(feature|Like) & P(feature|Dislike) \n",
    "        # for each feature with beta=0.1\n",
    "        like_m_f['pL'] = (like_m_f['like'] + 0.1) / (like + 2*(0.1))\n",
    "        like_m_f['pDL'] = (like_m_f['dislike'] + 0.1) / (dislike + 2*(0.1))\n",
    "\n",
    "        # Iterate over items to be predicted\n",
    "        for x2 in pref_known_test[m]:\n",
    "            like_m_l = pd.DataFrame(data=None, columns=['feature'])\n",
    "            # Iterate over features of item, x2 and add feature, z2 \n",
    "            for z2 in np.array(features.loc[x2]):\n",
    "                if z2 is not None:\n",
    "                    like_new2 = pd.DataFrame([[z2]],columns=['feature'])\n",
    "                    like_m_l = like_m_l.append(like_new2)\n",
    "        \n",
    "            # Filter down to unique feature list\n",
    "            like_m_l_f = like_m_l.feature.unique()\n",
    "\n",
    "            # Add features and its conditional probabilities that are newly compiled \n",
    "            # in test data as 0 for like & dislike\n",
    "            for w in like_m_l_f:\n",
    "                if w not in like_m_f.index:\n",
    "                    like_new = pd.DataFrame([[0,0,0.1/(like+(2*0.1)),0.1/(dislike+(2*0.1))]],index=[w],columns=['like','dislike','pL','pDL'])\n",
    "                    like_m_f = like_m_f.append(like_new) \n",
    "        \n",
    "            # Compute P(Liked) * Product of P(feature|Like) for all features \n",
    "            pLiked = prob_like\n",
    "            for v in np.array(features.loc[x2]):\n",
    "                if v is not None:\n",
    "                    pLiked = pLiked * like_m_f.loc[str(v)]['pL']\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            # Compute P(Disliked) * Product of P(feature|Dislike) for all features \n",
    "            pDisliked = prob_dislike\n",
    "            for v2 in np.array(features.loc[x2]):\n",
    "                if v2 is not None:\n",
    "                    pDisliked = pDisliked * like_m_f.loc[str(v2)]['pDL']\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "            # Compute P1 as P(Liked/features) & P2 as P(Disliked/features) \n",
    "            P1 = pLiked / (pLiked+pDisliked)\n",
    "            P2 = pDisliked / (pLiked+pDisliked)\n",
    "            val = np.log(P1/P2)\n",
    "            # Normalize to 1-5 rating scale\n",
    "            rat = 1/(1+np.exp(-val))\n",
    "            rating = rat*4 +1 \n",
    "            rating\n",
    "\n",
    "            # Assign rating as prediction for item, x2 for user m\n",
    "            test_pred.loc[train.index[m]][x2] = rating\n",
    "\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Based Recommender for Restaurant in Toronto took 8280.69850421 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_pred = content_based_recommender(train9, test9, features_filt)\n",
    "print 'Content Based Recommender for Restaurant in Toronto took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Based Recommender for Restaurant in Phoenix took 7056.17153001 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_pred_ph = content_based_recommender(train9_ph, test9_ph, features_filt_ph)\n",
    "print 'Content Based Recommender for Restaurant in Phoenix took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based Recommenders Sytems for Toronto Mean Absolute Error: 1.5151\n",
      "Content-Based Recommenders Sytems for Phoenix Mean Absolute Error: 1.4289\n"
     ]
    }
   ],
   "source": [
    "test_pred_0 = test_pred.fillna(0)\n",
    "test_pred_0_mat = test_pred_0.as_matrix()\n",
    "pred_list = list(test_pred.values.ravel())\n",
    "actual_list = list(test9.values.ravel())\n",
    "# len([x for x in pred_list if x>0])\n",
    "# len([x for x in actual_list if x>0])\n",
    "test_pred_0_ph = test_pred_ph.fillna(0)\n",
    "test_pred_0_mat_ph = test_pred_0_ph.as_matrix()\n",
    "pred_list_ph = list(test_pred_ph.values.ravel())\n",
    "actual_list_ph = list(test9_ph.values.ravel())\n",
    "# len([x for x in pred_list_ph if x>0])\n",
    "# len([x for x in actual_list_ph if x>0])\n",
    "print 'Content-Based Recommenders Sytems for Toronto Mean Absolute Error: ' + str(round(mae(test_pred_0_mat,test9_0_mat),4))\n",
    "print 'Content-Based Recommenders Sytems for Phoenix Mean Absolute Error: ' + str(round(mae(test_pred_0_mat_ph,test9_0_mat_ph),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
